# Protein Secondary Structure Prediction Using Contact Maps
-------------------------------------------------------

#### In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3- and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on Accuracy and F1-scores.

<!-- display the image -->
<p align="center">
  <img src="assets\architecture.jpg" width="400" height="300">

### Installing

1. Fetch the PDB files
```
python scripts\preprocessing_scripts\preprocess.py
```
2. Calculation the distances between c_alpha items in the pdb files
```
python scripts\preprocessing_scripts\distance.py
```

The following program has been tested using python 3.9.2.
Using `run.sh` will create and activate a virtualenv, install all necessary
dependencies and run a test program to ensure that you can import all the
modules.
1. To download the pdb files run the script 
<!-- insert link https://www.rcsb.org/docs/programmatic-access/batch-downloads-with-shell-script --> 
<>

```
# Run from the parent directory.
CUDA_VISIBLE_DEVICES=0 python scripts/train/MultiModal/train_with2lr.py

```

To run the provided code, use this virtualenv:

```
source /tmp/adversarial_robustness_venv/bin/activate
```

You may want to edit `requirements.txt` before running `run.sh` if GPU support
is needed (e.g., use `jaxline==0.1.67+cuda111`). See JAX's installation
[instructions](https://github.com/google/jax#installation) for more details.

### Using pre-trained models

Once downloaded, a model can be evaluated by running the `eval.py` script in
either the `jax` or `pytorch` folders. E.g.:

```
cd jax
python3 eval.py \
  --ckpt=${PATH_TO_CHECKPOINT} --depth=70 --width=16 --dataset=cifar10
```

These models are also directly available within
[RobustBench](https://github.com/RobustBench/robustbench#model-zoo-quick-tour)'s
model zoo.

### Training your own model

We also provide a training pipeline that reproduces results from both
publications. This pipeline uses [Jaxline](https://github.com/deepmind/jaxline)
and is written using [JAX](https://github.com/google/jax) and
[Haiku](https://github.com/deepmind/dm-haiku). To train a model, modify the
configuration in the `get_config()` function of `jax/experiment.py` and issue
the following command from within the virtualenv created above:

```
cd jax
python3 train.py --config=experiment.py
```

The training pipeline can run with multiple worker machines and multiple devices
(either GPU or TPU). See [Jaxline](https://github.com/deepmind/jaxline) for more
details.

We do not provide a PyTorch implementation of our training pipeline. However,
you may find one on GitHub, e.g.,
[adversarial_robustness_pytorch](https://github.com/imrahulr/adversarial_robustness_pytorch)
(by Rahul Rade).

## Datasets

### Extracted dataset

Gowal et al. (2020) use samples extracted from
[TinyImages-80M](https://groups.csail.mit.edu/vision/TinyImages/).
Unfortunately, since then, the official TinyImages-80M dataset has been
withdrawn (due to the presence of offensive images). As such, we cannot provide
a download link to our extrated data until we have manually verified that all
extracted images are not offensive. If you want to reproduce our setup, consider
the generated datasets below. We are also happy to help, so feel free to reach
out to Sven Gowal directly.

### Generated datasets

Rebuffi et al. (2021) and Gowal et al. (2021) use samples generated by a
Denoising Diffusion Probabilistic Model
[(DDPM; Ho et al., 2020)](https://arxiv.org/abs/2006.11239)
to improve robustness. The DDPM is solely trained on the original training data
and does not use additional external data. The following table links to datasets
of 1M **generated** samples for CIFAR-10, CIFAR-100 and SVHN.

| dataset | model | size | link |
|---|---|:---:|:---:|
| CIFAR-10 | DDPM | 1M | [npz](https://storage.googleapis.com/dm-adversarial-robustness/cifar10_ddpm.npz) |
| CIFAR-100 | DDPM | 1M | [npz](https://storage.googleapis.com/dm-adversarial-robustness/cifar100_ddpm.npz) |
| SVHN | DDPM | 1M | [npz](https://storage.googleapis.com/dm-adversarial-robustness/svhn_ddpm.npz) |

To load each dataset, use NumPy. E.g.:

```
npzfile = np.load('cifar10_ddpm.npz')
images = npzfile['image']
labels = npzfile['label']
```
<!-- insert image  -->

![Results Table](assets\Result.png)