{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch_geometric.loader import DataLoader \n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2primary = {0: 'A', 1: 'C', 2: 'D', 3: 'E', 4: 'F', 5: 'G', 6: 'H', 7: 'I', 8: 'K', 9: 'L', 10: 'M', 11: 'N', 12: 'P', 13: 'Q', 14: 'R', 15: 'S', 16: 'T', 17: 'V', 18: 'W', 19: 'Y', 20: 'X'}\n",
    "int2second = {0: 'G', 1: 'H', 2: 'I', 3: 'B', 4: 'E', 5: 'S', 6: 'T', 7: 'C'}\n",
    "\n",
    "\n",
    "class ProteinDataset(InMemoryDataset):\n",
    "    def __init__(self,\n",
    "                 root='/Data/deeksha/disha/ProtTrans/data/adjacency_data/',\n",
    "                 transform=None,\n",
    "                 pre_transform=None):\n",
    "        super(ProteinDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['g_data_train_short_q3.pkl']\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data_q3.pt']\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        with open(self.raw_paths[0], 'rb') as f:\n",
    "            data = pkl.load(f)\n",
    "\n",
    "        primary_seqs, seq_length, secondary_seqs, adjacency_matrices = [], [], [], []\n",
    "        for k, v in data.items():\n",
    "            primary_seqs.append(v[0])\n",
    "            seq_length.append(v[-2])\n",
    "            argmax = np.argmax(v[1], axis=1)\n",
    "            secondary_seqs.append(argmax)\n",
    "            adj = data[k][-1]\n",
    "            adj = np.vstack((adj.row, adj.col))\n",
    "            adjacency_matrices.append(adj)\n",
    "        \n",
    "        primary_seqs = np.array(primary_seqs)\n",
    "        seq_length = np.array(seq_length)\n",
    "        secondary_seqs = np.array(secondary_seqs)\n",
    "        # adjacency_matrices = np.array(adjacency_matrices)\n",
    "        seq_length = torch.tensor(seq_length, dtype=torch.long)\n",
    "        data_list = []\n",
    "        for i in tqdm(range(len(primary_seqs))):\n",
    "            x = torch.tensor(primary_seqs[i], dtype=torch.float)\n",
    "            y = torch.tensor(secondary_seqs[i], dtype=torch.long)\n",
    "            edge_index = torch.tensor(adjacency_matrices[i], dtype=torch.long)\n",
    "            data = Data(x=x, y=y, edge_index=edge_index, seq_len=seq_length[i])\n",
    "            data_list.append(data)\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ProteinDataset()\n",
    "train_dataset = dataset[:10000]\n",
    "val_dataset = dataset[10000:]\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i.seq_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_feature=21, hidden_channels=128, num_classes=3):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(input_feature, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = x.float()\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN().to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, out, target, seq_len):\n",
    "        out = out.view(len(seq_len), -1, 3)\n",
    "        target = target.view(len(seq_len), -1)\n",
    "        loss=0\n",
    "        for i in range(len(seq_len)):\n",
    "            o = out[i][:seq_len[i]]\n",
    "            t = target[i][:seq_len[i]]\n",
    "            loss += nn.CrossEntropyLoss()(o, t)\n",
    "        return loss/len(seq_len)\n",
    "        \n",
    "\n",
    "def accuracy(out, labels, seq_length):\n",
    "    acc = 0\n",
    "    out = out.view(len(seq_length), -1, 3)\n",
    "    labels = labels.view(len(seq_length), -1)\n",
    "    for o, t, l in zip(out, labels, seq_length):\n",
    "        o = o[:l]\n",
    "        t = t[:l]\n",
    "        acc += (o.argmax(1) == t).sum().item() / l\n",
    "\n",
    "    return (acc / len(seq_length)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = CrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    for data in tqdm(train_loader): \n",
    "         data = data.to(device)\n",
    "         out = model(data.x, data.edge_index)  \n",
    "         seq_length = torch.tensor(data.seq_len).to('cuda')\n",
    "        #  print(seq_length)\n",
    "         loss = criterion(out, data.y, seq_length)  \n",
    "         loss.backward() \n",
    "         optimizer.step()  \n",
    "         optimizer.zero_grad() \n",
    "\n",
    "def test(model, criterion, loader, accuracy, device):\n",
    "     model.eval()\n",
    "\n",
    "     total_accuracy, losses = 0, 0\n",
    "     for data in tqdm(loader):\n",
    "         data = data.to(device) \n",
    "         out = model(data.x, data.edge_index)  \n",
    "        # load the seq length list to cuda\n",
    "        #  print(type(data.seq_len))\n",
    "        #  seq_length = torch.tensor(data.seq_len).to('cuda')\n",
    "        #  print(seq_length)\n",
    "         seq_length = data.seq_len\n",
    "        #  print(seq_length, len(seq_length))\n",
    "         loss = criterion(out, data.y, seq_length)  \n",
    "         losses += loss.item() \n",
    "         total_accuracy += accuracy(out, data.y, seq_length)    \n",
    "     return total_accuracy / len(loader), losses / len(loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "device='cuda'\n",
    "for epoch in range(2):\n",
    "    train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_acc, train_loss = test(model, criterion, val_loader, accuracy, device)\n",
    "    test_acc, test_loss = test(model, criterion, val_loader, accuracy, device)\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_loss_list.append(test_loss)\n",
    "    test_acc_list.append(test_acc)\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "# save the model\n",
    "torch.save(model.state_dict(), '/Data/deeksha/disha/ProtTrans/scripts/train/model/gcn_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_loss = test(model, criterion, val_loader, accuracy, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_acc_list)\n",
    "plt.plot(train_loss_list, label='train_loss')\n",
    "plt.plot(test_loss_list, label='test_loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'/Data/deeksha/disha/ProtTrans/scripts/train/model/gcn_loss_q3.png')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(train_acc_list, label='train_acc')\n",
    "plt.plot(test_acc_list, label='test_acc')\n",
    "plt.legend()\n",
    "plt.savefig(f'/Data/deeksha/disha/ProtTrans/scripts/train/model/gcn_acc_q3.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2primary = {0: 'A', 1: 'C', 2: 'D', 3: 'E', 4: 'F', 5: 'G', 6: 'H', 7: 'I', 8: 'K', 9: 'L', 10: 'M', 11: 'N', 12: 'P', 13: 'Q', 14: 'R', 15: 'S', 16: 'T', 17: 'V', 18: 'W', 19: 'Y', 20: 'X'}\n",
    "# int2second = {0: 'G', 1: 'H', 2: 'I', 3: 'B', 4: 'E', 5: 'S', 6: 'T', 7: 'C'}\n",
    "int2second = {0: 'H', 1: 'E', 2: 'C'}\n",
    "model.eval()\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "primary_seqs, predicted_secondary, actual_secondary = [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader):\n",
    "        data = batch.to(device)\n",
    "        out = model(data.x, data.edge_index)\n",
    "        seq_len = data.seq_len\n",
    "        pred = out.argmax(dim=1)[:seq_len]\n",
    "        primary_feats = data.x.cpu().numpy().argmax(axis=1)[:seq_len]\n",
    "        primary_seqs.append(primary_feats)\n",
    "        actual_secondary.append(data.y.cpu().numpy()[:seq_len])\n",
    "        predicted_secondary.append(pred.cpu().numpy())\n",
    "        if len(predicted_secondary) == 10:\n",
    "            break\n",
    "\n",
    "    predicted_secondary_alpha= [[int2second[aa] for aa in sample] for sample in predicted_secondary]\n",
    "    actual_secondary_alpha = [[int2second[aa] for aa in sample] for sample in actual_secondary]\n",
    "    primary_seqs_alpha = [[int2primary[aa] for aa in sample] for sample in primary_seqs]\n",
    "\n",
    "with open('demo.txt', 'w') as f:\n",
    "    for i in range(len(predicted_secondary_alpha)):\n",
    "        f.write(f'Primary: {\" \".join(primary_seqs_alpha[i])}\\n')\n",
    "        f.write(f'Predicted: {\" \".join(predicted_secondary_alpha[i])}\\n')\n",
    "        f.write(f'Actual: {\" \".join(actual_secondary_alpha[i])}\\n\\n')\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pssp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
