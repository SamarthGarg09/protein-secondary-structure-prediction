{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pdb file preprocessing related code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"/Data/deeksha/disha/ProtTrans/data/list_file.txt\") as f:\n",
    "    data = f.read().split(\",\")\n",
    "len(data)\n",
    "data = np.array(data)\n",
    "np.unique(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "'''\n",
    "Extract the files which were left to download and store them to new file.           \n",
    "'''\n",
    "f_names = os.listdir(\"/Data/deeksha/disha/pdbid_files\")\n",
    "new_fn_name = []\n",
    "for f_name in f_names:\n",
    "    _fname = f_name.split('.')[0]\n",
    "    new_fn_name.append(_fname)\n",
    "len(new_fn_name), new_fn_name[0]\n",
    "\n",
    "data, new_fn_name = set(data), set(new_fn_name)\n",
    "\n",
    "unique_f_name = list(data.difference(new_fn_name))\n",
    "print(len(unique_f_name))\n",
    "unique_f_name[0]\n",
    "\n",
    "with open(\"/Data/deeksha/disha/ProtTrans/scripts/list_file2.txt\", 'w') as f2:\n",
    "    f2.write(\",\".join(unique_f_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"/Data/deeksha/disha/ProtTrans/data/dic_seq.pkl\", 'rb') as f:\n",
    "    data = pkl.load(f)\n",
    "len(data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['1wd3']['MET'][-1]\n",
    "# data.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------\n",
    "### DF-Final code \n",
    "--------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load npz file\n",
    "casp12 = np.load('/Data/deeksha/disha/ProtTrans/data/dataset_npz_files/CASP12_HHblits.npz', allow_pickle=True)\n",
    "cb513 = np.load(\"/Data/deeksha/disha/ProtTrans/data/dataset_npz_files/CB513_HHblits.npz\", allow_pickle=True)\n",
    "train = np.load(\"/Data/deeksha/disha/ProtTrans/data/dataset_npz_files/Train_HHblits.npz\", allow_pickle=True)\n",
    "ts115 = np.load(\"/Data/deeksha/disha/ProtTrans/data/dataset_npz_files/TS115_HHblits.npz\", allow_pickle=True)\n",
    "\n",
    "# extract data\n",
    "casp12_data = casp12['data']\n",
    "cb513_data = cb513['data']\n",
    "train_data = train['data']\n",
    "ts115_data = ts115['data']\n",
    "\n",
    "casp12_pdb = casp12['pdbids']\n",
    "cb513_pdb = cb513['pdbids']\n",
    "train_pdb = train['pdbids']\n",
    "ts115_pdb = ts115['pdbids']\n",
    "\n",
    "# print shapes of all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shapes of all data\n",
    "print(casp12_data.shape)\n",
    "# print(casp12_pdb.shape)\n",
    "print(cb513_data.shape)\n",
    "# print(cb513_pdb.shape)\n",
    "print(train_data.shape)\n",
    "# print(train_pdb.shape)\n",
    "print(ts115_data.shape)\n",
    "# print(ts115_pdb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract [0:20] Amino Acids and [57:65] Q8 codes\n",
    "primary_casp12 = casp12_data[:,:, :20]\n",
    "q8_casp12 = casp12_data[:,:, 57:65]\n",
    "\n",
    "primary_cb513 = cb513_data[:,:, :20]\n",
    "q8_cb513 = cb513_data[:,:, 57:65]\n",
    "\n",
    "primary_train = train_data[:,:, :20]\n",
    "q8_train = train_data[:,:, 57:65]\n",
    "\n",
    "primary_ts115 = ts115_data[:,:, :20]\n",
    "q8_ts115 = ts115_data[:,:, 57:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_train.shape, q8_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding primary and secondary sequence from one-hot to alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B', 'C', 'E', 'G', 'H', 'I', 'S', 'T'\n",
    "int2second = {0: 'G', 1: 'H', 2: 'I', 3: 'B', 4: 'E', 5: 'S', 6: 'T', 7: 'C'}\n",
    "encoded_second_seqs, max_sample_len = [], []\n",
    "\n",
    "for ex in tqdm(q8_train):\n",
    "    seq = \"\"\n",
    "    for aa in ex:\n",
    "        if sum(aa) == 0:\n",
    "            # seq += \"X\"\n",
    "            continue\n",
    "        else:\n",
    "            seq += int2second[np.argmax(aa)]\n",
    "    max_sample_len.append(len(seq))\n",
    "    encoded_second_seqs.append(seq)\n",
    "\n",
    "# encoded_second_seqs = np.array(encoded_second_seqs)\n",
    "# encoded_second_seqs[10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2primary = {0: 'A', 1: 'C', 2: 'D', 3: 'E', 4: 'F', 5: 'G', 6: 'H', 7: 'I', 8: 'K', 9: 'L', 10: 'M', 11: 'N', 12: 'P', 13: 'Q', 14: 'R', 15: 'S', 16: 'T', 17: 'V', 18: 'W', 19: 'Y', 20: 'X'}\n",
    "\n",
    "# {0: 'H', 1: 'B', 2: 'E', 3: 'G', 4: 'I', 5: 'T', 6: 'S', 7: 'C', 8:'X'}\n",
    "encoded_primary_seqs = []\n",
    "\n",
    "for i, ex in enumerate(tqdm(primary_train)):\n",
    "    ex = ex[:max_sample_len[i]]\n",
    "    seq = \"\"\n",
    "    for i, aa in enumerate(ex):\n",
    "        if sum(aa) == 0:\n",
    "            seq += 'X'\n",
    "            # continue\n",
    "        else:\n",
    "            seq += int2primary[np.argmax(aa)]\n",
    "    encoded_primary_seqs.append(seq)\n",
    "\n",
    "# encoded_primary_seqs = np.array(encoded_primary_seqs)\n",
    "encoded_primary_seqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_primary_seqs[0], encoded_second_seqs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"input\": encoded_primary_seqs, \"dssp8\": encoded_second_seqs})\n",
    "df['input'] = df.apply(lambda x: \" \".join(list(x['input'])), axis=1)\n",
    "df['dssp8'] = df.apply(lambda x: \" \".join(list(x['dssp8'])), axis=1)\n",
    "df['pdbid'] = train_pdb\n",
    "df['pdbid'] = df.apply(lambda x: x['pdbid'].split('-')[0], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/Data/deeksha/disha/ProtTrans/data/csv_files/Train_HHblits.csv\"\n",
    "cb513_path = \"/Data/deeksha/disha/ProtTrans/data/csv_files/CB513_HHblits.csv\"\n",
    "ts115_path = \"/Data/deeksha/disha/ProtTrans/data/csv_files/TS115_HHblits.csv\"\n",
    "casp12_path = \"/Data/deeksha/disha/ProtTrans/data/csv_files/CASP12_HHblits.csv\"\n",
    "\n",
    "df_train_alpha = pd.read_csv(train_path)\n",
    "# df_train_alpha = df_train_alpha[['input', ' dssp8']]\n",
    "df_train_alpha.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape of df_train_alpha: {df_train_alpha.shape}')\n",
    "print(f'shape of df: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_alpha[' dssp8'].isin(df['dssp8']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_alpha.rename(columns={' dssp8': 'dssp8'}, inplace=True)\n",
    "df = df.drop_duplicates(subset=['dssp8'])\n",
    "df_train_alpha = df_train_alpha.drop_duplicates(subset=['dssp8'])\n",
    "df.shape, df.dssp8.nunique(), df.input.nunique(), df_train_alpha.shape, df_train_alpha['dssp8'].nunique(), df_train_alpha.input.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df3 = pd.merge(df_train_alpha, df, on=['dssp8'], how='inner')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.drop_duplicates(subset=['dssp8'])\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df3.input_x == df3.input_y).value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/Data/deeksha/disha/ProtTrans/data/final_excel_files/df_final.xlsx\"\n",
    "cb513_path = \"/Data/deeksha/disha/ProtTrans/data/final_excel_files/cb513_final.xlsx\"\n",
    "ts115_path = \"/Data/deeksha/disha/ProtTrans/data/final_excel_files/ts115_final.xlsx\"\n",
    "casp12_path = \"/Data/deeksha/disha/ProtTrans/data/final_excel_files/casp12_final.xlsx\"\n",
    "\n",
    "df3.to_excel(train_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"/Data/deeksha/disha/ProtTrans/data/final_excel_files/df_final.xlsx\")\n",
    "\n",
    "df_train = df[:int(0.9*df.shape[0])]\n",
    "df_test = df[int(0.9*df.shape[0]):]\n",
    "\n",
    "# save train and test data\n",
    "df_train.to_excel(\"/Data/deeksha/disha/ProtTrans/data/final_excel_files/df_train.xlsx\", index=False)\n",
    "df_test.to_excel(\"/Data/deeksha/disha/ProtTrans/data/final_excel_files/df_test.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pssp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
